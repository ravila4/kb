{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Ricardo's Notes","n":0.707},"1":{"v":"\n## Welcome to my Personal Knowledge Base!\n\n![](/assets/images/profile.jpg)\n\nI am a bioinformatics programmer, currently working at the Scripps Research Institute.\n\nI enjoy learning about technology, science, and art. ðŸ’» ðŸ§¬ ðŸŽ¨\n\nThis is my attempt at organizing and offloading knowledge from topics that I come across often, but not necessarily use on a daily basis, or things that I am in the process of learning.\n\n## My Links\n\n[GitHub](https://github.com/ravila4)\n\n[LinkedIn](https://linkedin.com/in/ravila4)","n":0.126}}},{"i":2,"$":{"0":{"v":"VS Code","n":0.707},"1":{"v":"\nHow to do some useful things with VS Code.\n\n\n## Useful keyboard shortcuts\n\n### Vim extension shortcuts:\n\n| Shortcut | Description                                     |\n| -------- | ----------------------------------------------- |\n| `gh`     | Show type of the symbol under the cursor        |\n| `gd`     | Go to definition of the symbol under the cursor |\n\n### VSCode shortcuts:\n\n| Shortcut           | Description                       |\n| ------------------ | --------------------------------- |\n| `Ctrl + Shift + N` | New empty window                  |\n| `Alt + Shift + P`  | Open recent projects              |\n| `Alt + â†“â†‘`         | Move the current line up or down. |\n| `F2`               | Rename the current symbol         |\n\n## Debugging\n\nTo setup custom debugging, edit the project;s `.vscode/launch.json` file.\n\n### Debug a python module\n\n```json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python: Module\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"myproject.module_name\",\n            \"args\": [\n                \"arg1\",\n                \"arg2\"\n            ],\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n```\n\n### Debug with a custom bash command\n\n```json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Bash: Command\",\n            \"type\": \"shell\",\n            \"request\": \"launch\",\n            \"command\": \"your_command\",\n            \"args\": [\n                \"arg1\",\n                \"arg2\"\n            ],\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n```\nExample from mygeneset.info's debug environment:\n\n```json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n\n        {\n            \"name\": \"Web Component\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"index.py\",\n            \"console\": \"integratedTerminal\",\n            \"args\": [\n                \"--debug\"\n            ],\n            \"cwd\": \"${workspaceFolder}/src\",\n        },\n        {\n            \"name\": \"Hub\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"module\": \"bin.hub\",\n            \"console\": \"integratedTerminal\",\n            \"cwd\": \"${workspaceFolder}/src\",\n        },\n        {\n            \"name\": \"Current File\",\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"console\": \"integratedTerminal\",\n        }\n    ]\n}\n```\n\n## Testing\n\n### Configuring testing\n\nAdd to workspace settings.json:\n\n```json\n{\n    \"python.testing.autoTestDiscoverOnSaveEnabled\": true,\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestPath\": \"pytest\"\n    \"python.testing.pytestArgs\": [\n        \"path/to/test/dir\",\n    ]\n}\n```\n\n## Technical Issues\n\n### Fedora installation\n\nTo install without using flatpak:\n\n```bash\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc\n\ncat <<EOF | sudo tee /etc/yum.repos.d/vscode.repo\n[code]\nname=Visual Studio Code\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\nenabled=1\ngpgcheck=1\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\nEOF\n```\n\n### Escaping the Flatpak sandbox\n\nFlatpak installations start a sandboxed shell that doesn't have access to the host's system packages.\n\nTo fix this, I originally used `flatpak-spawn` to start a shell. The settings under `terminal.integrated.profiles.linux` are:\n\n```json\n    \"Host: Bash (new pty)\": {\n        \"path\": \"/usr/bin/flatpak-spawn\",\n        \"args\": [\n            \"--env=TERM=vscode\",\n            \"--host\",\n            \"script\",\n            \"--quiet\",\n            \"/dev/null\"\n        ]\n    },\n    \"Host: ZSH (new pty)\": {\n        \"path\": \"/usr/bin/flatpak-spawn\",\n        \"args\": [\n            \"--env=TERM=vscode\",\n            \"--env=SHELL=zsh\",\n            \"--host\",\n            \"script\",\n            \"--quiet\",\n            \"/dev/null\"\n        ]\n    }\n```\n\nA better solution was to use 'host-spawn`: https://github.com/1player/host-spawn to start a shell:\n\n```json\n\n\"terminal.integrated.profiles.linux\": {\n    \"Host: Bash (host-spawn)\": {\n        \"path\": \"/home/ravila/.local/bin/host-spawn\",\n        \"args\": [\n            \"bash\"\n        ]\n    }\n}\n```","n":0.054}}},{"i":3,"$":{"0":{"v":"Tags","n":1}}},{"i":4,"$":{"0":{"v":"SmartAPI","n":1}}},{"i":5,"$":{"0":{"v":"ES8","n":1}}},{"i":6,"$":{"0":{"v":"BioThings","n":1}}},{"i":7,"$":{"0":{"v":"MyChem","n":1}}},{"i":8,"$":{"0":{"v":"MyChem Paper","n":0.707}}},{"i":9,"$":{"0":{"v":"Biothings Auth","n":0.707}}},{"i":10,"$":{"0":{"v":"R","n":1}}},{"i":11,"$":{"0":{"v":"Python","n":1}}},{"i":12,"$":{"0":{"v":"Tricks","n":1},"1":{"v":"\n## 1) Adding folder to path\n\nSometimes a module that you want to import is found in a parent directory from where the Python executable is, this trick makes appends any path to the current namespace.\n\n```python\n# Add a folder to path. Useful to call a module that is in a parent directory:\nimport sys\nsys.path.append(\"../\")\n```\n\n## 2) Find the location of a module\n\n```python\nimport mymodule\nmymodule.__file__\n```\n\n## 3) Python profiling with cProfile\n\nIn a terminal, run:\n\n```bash\npython -m cProfile -o tmp.prof <your_script.py>\n```\nVisualise output with snakeviz:\n\n```bash\nsnakeviz tmp.prof\n```\n\n","n":0.114}}},{"i":13,"$":{"0":{"v":"Requests","n":1},"1":{"v":"\n\n\n## Basic Requests\n\n* **GET**: `response = requests.get('https://api.github.com')`\n* **POST**: `response = requests.post('https://api.github.com')`\n\n## Advanced requests\n\n### Parameters\n\nPassing query string parameters to a GET request using `params`:\n\n```python\nrequests.get(\n    'https://api.github.com/search/repositories',\n    params=[('q', 'requests+language:python')]\n)\n```\n\nOr in a single line:\n\n```python\nresponse=requests.get(\n    'https://api.github.com/search/repositories?q=requests+language:python')\n```\n\n### Body\n\nSimilarly, the `data` argument takes a request body:\n\n```python\npayload = json.dumps({\n    \"name\" : \"Test private geneset\",\n    \"genes\": [\"1001\"],\n    \"is_public\": \"true\"\n})\n\nurl = \"https://mygeneset.info/v1/user_geneset\"\n\nresponse = requests.request(\"POST\", url, data=payload)\n```\n\n### Headers\n\nHeaders can be passed to the `headers` argument.\n\n#### Accept\n\n The Accept header tells the server what content types your application can handle. It has the format: `Accept: <MIME_type>/<MIME_subtype>`\n\nCommon Accept values:\n\n* `headers={'Accept': '*/*'}`\n* `headers={'Accept': 'text/html'}`\n* `headers={'Accept': 'application/json'}`\n* `headers={'Accept': 'image/jpeg'}`\n\n#### Cookies\n\n```python\nheaders={Cookie: 'user=\"2|1:0|10:1651076373|4:user|284:eyJ1c2VybmFtZSI6IC...\"'}\n```\n### Other common request header fields\n\n* Authorization: `Authorization: BASIC <credentials>`\n* User-Agent: `User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:50.0) `\n* Accept-Encoding\n\n## Responses\n\n### Status\n* `response.status_code`\n\n### Content\n* `response.content`: raw bytes payload\n* `response.text`: string\n* `response.json()`: JSON object\n\n## Headers\n\n* `response.headers`\n\n\n## SSL validation\n\n```python\nrequests.get('https://api.github.com', verify=False\n```\n\n## Setting Custom Timeouts\n\n```python\nrequests.get('https://api.github.com', timeout=5.0)\n```\n\nSet request/connect timeouts:\n\n```\nrequests.get('https://api.github.com', timeout=(2, 5))\n```\n\n\n\n\n\n### Checking for a successful response:\n\n```python\nif response.status_code != 200:\n    print('Error!')\n```\n\nOr even better:\n\n`response.raise_for_status()`\n\n## Hooks\n\n```python\nhttp = requests.Session()\n\nassert_status_hook = (\n    lambda response, *args, **kwargs: response.raise_for_status()\n)\nhttp.hooks[\"response\"] = [assert_status_hook]\n\nhttp.get(\"https://api.github.com/user/repos?page=1\")\n```\n\n\n## Retries\n\nWe might be tempted to do:\n\n```python\nimport time\nimport requests\n\n\ndef get(url):\n    try:\n        return requests.get(url)\n    except Exception:\n        # sleep for a bit\n        time.sleep(1)\n        # try again\n        return get(url)\n```\n\nThis can fail for many reasons, including incorrect URL, 500 errors, etc.\n\nA better soluton:\n\n``` python\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\n\ndef requests_retry_session(\n    retries=3,\n    backoff_factor=0.3,\n    status_forcelist=(500, 502, 504),\n    session=None,\n):\n    session = session or requests.Session()\n    retry = Retry(\n        total=retries,\n        read=retries,\n        connect=retries,\n        backoff_factor=backoff_factor,\n        status_forcelist=status_forcelist,\n    )\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    return session\n```","n":0.064}}},{"i":14,"$":{"0":{"v":"Pyspark","n":1},"1":{"v":"\n# Creating Dataframes\n\n## From a csv\n\n```python\ndf = spark.read.csv(\"data.csv\", header=True)\n```\n\n## From pandas\n\n```python\ndf = spark.createDataFrame(pandas_df)\n```\n\n# Selections\n\n## Selecting columns\n\n```python\ndf.select(\"column_name\")\n```\n\n## Filtering\n\nFilter rows to those that are not null\n    \n```python\ndf.filter(df.column_name.isNotNull())\n```\n\nFilter based on the length of an array column\n\n```python\ndf.filter(size(df.column_name) > 0)\n```\n\n## Count unique\n\n```python\ndf.select(\"column_name\").distinct().count()\n```\n\n# Transformations\n\n## Add a new column\n\nWith a constant value:\n\n```python\ndf.withColumn(\"new_column\", lit(1))\n```\n\nFrom another column:\n\n```python\ndf.withColumn(\"new_column\", df.column_name + 1)\n```\n## UDFs\n\nCreate a udf and apply it to a column:\n\n```python\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\ndef add_one(x):\n    return x + 1\n\nadd_one_udf = udf(add_one, IntegerType())\n\ndf.withColumn(\"new_column\", add_one_udf(df.column_name))\n```\n\n## GroupBy and Aggregate\n\nAggreagate a dataframe by counting the number of rows in each group:\n\n```python\ndf.groupBy(\"column_name\").count()\n```\n\nAggreagate a dataframe by counting the number of rows in each group, and sort by count:\n\n```python\ndf.groupBy(\"column_name\").count().orderBy(\"count\", ascending=False)\n```\n\nAggregate a dataframe by the number of unique values in a column:\n\n```python\ndf.groupBy(\"column_name\").agg(countDistinct(\"column_name\"))\n```\n\nGroup a dataframe by a column and aggregate by a udf\n\n```python\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\ndef add_one(x):\n    return x + 1\n\nadd_one_udf = udf(add_one, IntegerType())\n\ndf.groupBy(\"column_name\").agg(add_one_udf(\"column_name\"))\n```\n\n## Joining\n\nLeft join two dataframes on two common columns:\n\n```python\ndf1.join(\n    df2, [df1.column_name1 == df2.column_name1,\n          df1.column_name2 == df2.column_name2],\n    \"left\"\n)\n```\n","n":0.08}}},{"i":15,"$":{"0":{"v":"Pandas","n":1},"1":{"v":"\n## Creating DataFrames\n\nSpecify header while reading a tab-delimited file:\n\n```python\ncols = [\"col1\", \"col2\", \"col3\" ]\ndf = pd.read_csv(\"data.tab\", sep=\"\\t\", names=cols)\n```\n\n## Cleaning Data\n\nRename several DataFrame columns:\n\n```python\ndf = df.rename(columns = {\n    'col1 old name':'col1 new name',\n    'col2 old name':'col2 new name',\n    'col3 old name':'col3 new name',\n})\n```\n\nGet a report of all duplicate records in a DataFrame, based on specific columns:\n\n```python\ndupes = df[df.duplicated(['col1', 'col2', 'col3'], keep=False)]\n```\n\nRemove duplicates by column, keeping first entry:\n\n```python\ndf = df.drop_duplicates(subset='col', keep='first')\n```\n\nReset a DataFrame's index to continuous integers, without saving old index (eg. after deleting records).\n\n```python\ndf.reset_index(drop=True, inplace=True)\n```\n\nClean up missing values in multiple DataFrame columns:\n\n```python\ndf = df.fillna({\n    'col1': 'missing',\n    'col2': 999,\n    'col3': 0,\n    'col4': 'missing',\n})\n```\n\nRename several DataFrame columns:\n\n```python\ndf = df.rename(columns = {\n    'col1 old name':'col1 new name',\n    'col2 old name':'col2 new name',\n    'col3 old name':'col3 new name',\n})\n```\n\n\n## Exploring Data\n\nSort DataFrame on a column:\n```python\ndf.sort_values(by='col1', ascending=False)\n```\n\n## Indexing and Selecting Data\n\nAccessing an index from a multi-index DataFrame:\n\n```python\npandas.MultiIndex.get_level_values\n```\n\n## Grouping and Transforming Data\n\nGroup by two columns and count total in groups:\n\n```python\ndf.groupby(['col1', 'col2']).count()\n```\n\nGroup and then aggregate all columns with a function:\n\n```python\ndf.groupby('col').agg(function)\n```\n\nGroup and aggregate specific columns with specific functions:\n\n```python\ndf.groupby('col').agg({\"col1\": np.sum, \"col2\": pd.Series.nunique})\n```\n\nSpreadsheet-style pivot with aggregation:\n\n```python\ntable = pd.pivot_table(\n    df, values='D', index=['A', 'B'], columns=['C'], aggfunc=np.sum)\n```\n","n":0.074}}},{"i":16,"$":{"0":{"v":"Packaging","n":1},"1":{"v":"\n\n## Wheels\n\nWheels are Python's binary distribution format.\n\n### Build a Python wheel\n\n```\npython setup.py bdist_wheel\n```\n\n### Install a wheel\n\n```\npip install <wheel_file>\n```\n\n","n":0.236}}},{"i":17,"$":{"0":{"v":"Higher-Order Functions","n":0.707},"1":{"v":"\nHigher-order functions are functions that take other functions as input and return functions as output.\n\nSome examples of built-in higher-order function in Python include:  `map()`, `filter()`, and `reduce()`.\n\nThe `functools` module in Python also provides built-in higher-order functions and operations for working with them.\n\n## Higher-order functions and decorators\n\nDecorators are a type of higher-order function that extends the functionality of a given function.\n\nLet's start with a simple decorator example. In this simplest case let's decorate a function that takes no arguments:\n\n```python\n# This is a regular function\ndef hello():\n    print(\"Hello world\")\n\n# This is a decorator function:\ndef goodbye(func):\n    def inner():\n        func()\n        print (\"Goodbye world!\")\n    return inner\n```\n\nThe common pattern of decorator functions is that they contain an inner and an outer function. The job ouf the outer function is simply to return the inner function. The inner function calls and extends the input function.\n\nWhen we pass the `hello` function to the `goodbye` function, we get a new function:\n\n```python\nhello_goodbye = goodbye(hello)\nhello_goodbye()\n>>> Hello world\n>>> Goodbye world!\n```\n\nThe next step up in terms of complexity is adding arguments to the inner function:\n\n```python\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\ndef decorate(func):\n    def inner(name):\n        func(name)\n        print(\"It's nice to meet you!\")\n    return inner\n\ndecorate(greet)(\"Ricardo\")\n>>> Hello, Ricardo!\n>>> It's nice to meet you!\n```\n\nThat's not too hard! Notice that the syntax `decorate(greet)(\"Ricardo\")` sends the string \"Ricardo\" to the inner function of the `decorate` function.\n\nFinally we can add some syntactic sugar to our decorators by using Python's `@` operator. In this case, if we wanted to decorate the function `greet` above, we could write it like this:\n\n```python\ndef goodbye(func):\n    def inner(name):\n        func(name)\n        print(f\"Goodbye, {name}\")\n    return inner\n\n@goodbye\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\ngreet(\"Ricardo\")\n>>> Hello, Ricardo!\n>>> Goodbye, Ricardo\n```\n\nAs you can see, `@goodbye` essentially modifies the function to return the output of `goodbye(greet)`.\n\nThat is basically all there is to know about decorators!\n\n\n## Partial functions\n\nPartial functions are a type of higher-order function that extends functions by pre-defining part of their arguments.\n\nLet's say we have a function that takes two numeric arguments and sums them together:\n\n```python\ndef sum(a, b)\n    return a + b\n\nsum(1, 2)\n>>> 3\n```\n\nNow lets say we want to create a new version of this fuction where the argument `a` is a fixed value, say `5` , and we only need to input the argument `b`.\n\nThe `functools` module provides an easy way to do this:\n\n```python\nfrom functools import partial\n\nsum_five = partial(sum, 5)\nsum_five(2)  # Sums 2 + 5\n>>> 7\n```\n\nThe function `partial` takes a function as its first argument, and passes the rest of its arguments to that function. It returns a new function with a lower number of inputs. To learn how it does this, we can actually implement `partial` ourselves using decorators! We also need to learn about the special `*args` and `**kwargs` parameters.\n\n\nLet's start by implementing the sample above from scratch:\n\n```python\ndef input_five(func):\n    def inner(b):\n        return func(5, b)\n    return inner\n\ndef sum(a, b):\n    return a + b\n\nsum_five = input_five(sum)\nsum_five(4)\n>>> 9\n```\n\nAs you can see, decorators can be used to create partial functions! \nOf course, we could have used the decorator syntax `@input_five`, but I wanted to drive home the point that `sum_five` is a new function that takes less arguments than `sum`.\n\n## Variable arguments\n\nThe only issue with the implementation above is that sometimes you don't know how many arguments the input function will have. Say we instead passed a function that sums three numbers the function `input_five` above, in this case it will fail, because `func` will expect three arguments instead of two.\n\nTo solve this we can use the special parameter `*args`. `*args` is a function parameter that catches any arbitrary number of **un-named** input parameters as a list. Note that `args` is just a variable name (by convention we use the name `args` or `argv`), and `*` is the important part of this parameter. It is the operator `*` that denotes packing the input arguments into an iterable and assigning them to the variable that follows it.\n\nA simple function using `*args`:\n\n```python\ndef print_all_arguments(*args):\n    for arg in args:\n        print(arg)\n\nprint_all_arguments(1, 2, 3)\n>>> 1\n>>> 2\n>>> 3\n```\n\nFrom this, you can proably get an idea of how this is useful for building decorators that can be applied across functions with varying number of arguments.\nHere is the same partial function decorator using `*args`:\n\n```python\ndef input_five(func):\n    def inner(*args):\n        return func(5, *args)\n    return inner\n\ndef sum(a, b, c, d):\n    return a + b + c +d\n\nsum_five = input_five(sum)\nsum_five(1, 10, 100)\n>>> 116\n```\n\nTo wrap things up, let's learn about **kwargs:\n\n`**kwargs` is similar to `*args`, but catches **named** input parameters into a dictionary. Just as in `*args`, the important part is the operator `**` which denotes packing name-value pairs into a dictonary and assigning them to a variable named `kwargs`. The list of arguments can then be accessed through the dictionary's `.items()` method.\n\nAn example:\n\n```python\ndef print_all_arguments(**kwargs):\n    for k, v in kwargs.items():\n        print(f\"Argument {k} has the value {v}\")\n\nprint_all_arguments(a=1, b=2, c=3)\n>>> Argument a has the value 1\n>>> Argument b has the value 2\n>>> Argument c has the value 3\n```\n\nI leave it as an exercise to think how you can use `**kwargs` to build a decorator that can be applied to functions with varying number of named arguments.\n","n":0.035}}},{"i":18,"$":{"0":{"v":"Conda","n":1},"1":{"v":"\n## Environment Management\n\n### Updating all environment packages:\n\n```\nconda update --all\n```\n\n### Cloning an environment:\n\n```\nconda create -n new-env --clone base\n```\n\n### Exporting an environment:\n\n```\nconda env export | grep -v \"^prefix: \" > environment.yml\n```\n\n### Re-creating an environment from a file:\n\n```\nconda env create -f environment.yml\n```\n\n### Removing an environment\n\n```\nconda env remove -n ENV_NAME\n```\n\n","n":0.147}}},{"i":19,"$":{"0":{"v":"Linux","n":1}}},{"i":20,"$":{"0":{"v":"Wget","n":1},"1":{"v":"\nA software package ofor retriving files with HTTP, HTTPS, FTP, and FTPS.\n\n## Common use cases\n\nDownload and save with a new name\n\n`wget -O newfile.txt`\n","n":0.209}}},{"i":21,"$":{"0":{"v":"Vim","n":1}}},{"i":22,"$":{"0":{"v":"Nvim-R","n":1},"1":{"v":"\n## Nvim-R plugin\n\nVim plugin for R integration.\n\n- Start/Close R\n- Send lines, selection, paragraphs, functions, blocks, entire file to R, knitr, pdflatex\n- Object browser\n- Help browser\n- Completion\n- Object introspection\n\n## Key bindings\n\nNote: The `<LocalLeader>` is '\\\\' by default.\n\nNote: It is recommended the use of different keys for `<Leader>` and\n`<LocalLeader>` to avoid clashes between filetype plugins and general plugins\nkey binds. See `|filetype-plugins|`, `|maplocalleader|` and\n[Nvim-R-localleader](faq-and-tips#remap-the-localleader).\n\nTo use the plugin, open a `.R`, `.Rnw`, `.Rd`, `.Rmd` or `.Rrst` file with Vim and\ntype `<LocalLeader>rf`. Then, you will be able to use the plugin key bindings to\nsend commands to R.\n\nThis plugin has many key bindings, which correspond with menu entries. In the\nlist below, the backslash represents the `<LocalLeader>`. Not all menu items and\nkey bindings are enabled in all filetypes supported by the plugin (`r`,\n`rnoweb`, `rhelp`, `rrst`, `rmd`).\n\n### Start/Close\n\n| Action             | Default shortcut |\n| ------------------ | ---------------- |\n| Start R (default)  | \\\\rf             |\n| Start R (custom)   | \\\\rc             |\n| Close R (no save)  | \\\\rq             |\n| Stop (interrupt) R | :RStop           |\n\n### Send\n\n| Action                                            | Default shortcut |\n| ------------------------------------------------- | ---------------- |\n| File                                              | \\\\aa             |\n| File (echo)                                       | \\\\ae             |\n| File (open `.Rout`)                               | \\\\ao             |\n| Block (cur)                                       | \\\\bb             |\n| Block (cur, echo)                                 | \\\\be             |\n| Block (cur, down)                                 | \\\\bd             |\n| Block (cur, echo and down)                        | \\\\ba             |\n| Chunk (cur)                                       | \\\\cc             |\n| Chunk (cur, echo)                                 | \\\\ce             |\n| Chunk (cur, down)                                 | \\\\cd             |\n| Chunk (cur, echo and down)                        | \\\\ca             |\n| Chunk (from first to here)                        | \\\\ch             |\n| Function (cur)                                    | \\\\ff             |\n| Function (cur, echo)                              | \\\\fe             |\n| Function (cur and down)                           | \\\\fd             |\n| Function (cur, echo and down)                     | \\\\fa             |\n| Selection                                         | \\\\ss             |\n| Selection (echo)                                  | \\\\se             |\n| Selection (and down)                              | \\\\sd             |\n| Selection (echo and down)                         | \\\\sa             |\n| Selection (evaluate and insert output in new tab) | \\\\so             |\n| Send motion region                                | \\\\m{motion}      |\n| Paragraph                                         | \\\\pp             |\n| Paragraph (echo)                                  | \\\\pe             |\n| Paragraph (and down)                              | \\\\pd             |\n| Paragraph (echo and down)                         | \\\\pa             |\n| Line                                              | \\\\l              |\n| Line (and down)                                   | \\\\d              |\n| Line (and new one)                                | \\\\q              |\n| Left part of line (cur)                           | \\\\r\\<Left\\>      |\n| Right part of line (cur)                          | \\\\r\\<Right\\>     |\n| Line (evaluate and insert the output a comment)   | \\\\o              |\n| All lines above the current one                   | \\\\su             |\n\n### Command\n\n| Action                                          | Default shortcut |\n| ----------------------------------------------- | ---------------- |\n| List space                                      | \\\\rl             |\n| Clear console                                   | \\\\rr             |\n| Remove objects and clear console                | \\\\rm             |\n| Print (cur)                                     | \\\\rp             |\n| Names (cur)                                     | \\\\rn             |\n| Structure (cur)                                 | \\\\rt             |\n| View data.frame (cur) in new tab                | \\\\rv             |\n| View data.frame (cur) in horizontal split       | \\\\vs             |\n| View data.frame (cur) in vertical split         | \\\\vv             |\n| View head(data.frame) (cur) in horizontal split | \\\\vh             |\n| Run dput(cur) and show output in new tab        | \\\\td             |\n| Arguments (cur)                                 | \\\\ra             |\n| Example (cur)                                   | \\\\re             |\n| Help (cur)                                      | \\\\rh             |\n| Summary (cur)                                   | \\\\rs             |\n| Plot (cur)                                      | \\\\rg             |\n| Plot and summary (cur)                          | \\\\rb             |\n| Set working directory (cur file path)           | \\\\rd             |\n| Sweave (cur file)                               | \\\\sw             |\n| Sweave and PDF (cur file)                       | \\\\sp             |\n| Sweave, BibTeX and PDF (cur file) (Linux/Unix)  | \\\\sb             |\n| Knit (cur file)                                 | \\\\kn             |\n| Knit, BibTeX and PDF (cur file) (Linux/Unix)    | \\\\kb             |\n| Knit and PDF (cur file)                         | \\\\kp             |\n| Knit and Beamer PDF (cur file)                  | \\\\kl             |\n| Knit and HTML (cur file, verbose)               | \\\\kh             |\n| Knit and ODT (cur file)                         | \\\\ko             |\n| Knit and Word Document (cur file)               | \\\\kw             |\n| Markdown render (cur file)                      | \\\\kr             |\n| Spin (cur file) (only .R)                       | \\\\ks             |\n| Markdown render (cur file) (all in YAML)        | \\\\ka             |\n| Quarto render (cur file)                        | \\\\qr             |\n| Quarto preview (cur file)                       | \\\\qp             |\n| Quarto stop preview (all files)                 | \\\\qs             |\n| Open attachment of reference (Rmd, Rnoweb)      | \\\\oa             |\n| Open PDF (cur file)                             | \\\\op             |\n| Search forward (SyncTeX)                        | \\\\gp             |\n| Go to LaTeX (SyncTeX)                           | \\\\gt             |\n| Debug (function)                                | \\\\bg             |\n| Undebug (function)                              | \\\\ud             |\n| Build tags file (cur dir)                       | :RBuildTags      |\n\n### Edit\n\n| Action                              | Default shortcut |\n| ----------------------------------- | ---------------- |\n| Insert `<-`                         | \\_               |\n| Complete object name                | CTRL-X CTRL-O    |\n| Indent (line)                       | ==               |\n| Indent (selected lines)             | =                |\n| Indent (whole buffer)               | gg=G             |\n| Toggle comment (line, sel)          | \\\\xx             |\n| Comment (line, sel)                 | \\\\xc             |\n| Uncomment (line, sel)               | \\\\xu             |\n| Add/Align right comment (line, sel) | \\\\;              |\n| Go (next R chunk)                   | \\\\gn             |\n| Go (previous R chunk)               | \\\\gN             |\n\n### Object Browser\n\n| Action               | Default shortcut |\n| -------------------- | ---------------- |\n| Open/Close           | \\\\ro             |\n| Expand (all lists)   | \\\\r=             |\n| Collapse (all lists) | \\\\r-             |\n| Toggle (cur)         | Enter            |\n\n### Help\n\n| Action        | Default shortcut |\n| ------------- | ---------------- |\n| Help (plugin) |\n| Help (R)      | :Rhelp           |\n\n## Custom keybindings\n\nTo customize a key binding you could put something like this in your `vimrc`:\n\n```vim\nfunction! s:customNvimRMappings()\n   nmap <buffer> <Leader>sr <Plug>RStart\n   imap <buffer> <Leader>sr <Plug>RStart\n   vmap <buffer> <Leader>sr <Plug>RStart\nendfunction\naugroup myNvimR\n   au!\n   autocmd filetype r call s:customNvimRMappings()\naugroup end\n```\n\n## Commands\n\n### Star/Close R\n\n- RStart\n- RCustomStart\n- RClose\n- RSaveClose\n\n### Clear R console\n\n- RClearAll\n- RClearConsole\n\n### Edit R code\n\n- RSimpleComment\n- RSimpleUnComment\n- RToggleComment\n- RRightComment\n- RIndent\n- RNextRChunk\n- RPreviousRChunk\n\n### Send code to R console\n\n- RSendLine\n- RSendAboveLines\n- RSendSelection\n- RSendMotion\n- RSendSelAndInsertOutput\n- RSendMBlock\n- RSendParagraph\n- RSendFunction\n- RSendFile\n\n### Send command to R\n\n- RHelp\n- RPlot\n- RSPlot\n- RShowArgs\n- RShowEx\n- RShowRout\n- RObjectNames\n- RObjectPr\n- RObjectStr\n- RViewDF (view data.frame or other object)\n- RViewDFs (view object in split window)\n- RViewDFv (view object in vertically split window)\n- RViewDFa (view object in split window above the current one)\n- RDputObj\n- RSetwd\n- RSummary\n- RListSpace\n\n### Support to Sweave and knitr\n\n- RBibTeXK (Knitr)\n- RKnit\n- RMakeHTML\n- RMakePDFK (Knitr)\n- RMakePDFKb (.Rmd, beamer)\n- RMakeODT (.Rmd, Open document)\n- RMakeWord (.Rmd, Word document)\n- RMakeRmd (rmarkdown default)\n- RMakeAll (rmarkdown all in YAML)\n- ROpenPDF\n- RNextRChunk\n- RPreviousRChunk\n\n### Support to Quarto\n\n- RQuartoRender\n- RQuartoPreview\n- RQuartoStop\n","n":0.032}}},{"i":23,"$":{"0":{"v":"Utils","n":1},"1":{"v":"\n## grep\n\n### Line context\n\n-A NUM\n: Print n lines of trailing context after each match.\n\n-B NUM\n: Print n lines of leading context before each match.\n\n### Inline context\nLimit the context to N characters before and after the match:\n\n```bash\ngrep -o -E \".{0,10}wantedText.{0,10}\" input_file\n```\n\n\n## curl\n\nPronounced \"See URL\". Curl is useful for sending HTTP/HTTPS requests.\n\nCommon flags:\n\n-X\n: method. e.g. GET, POST, PUT, DELETE, etc.\n\n-H:\nheaders\n\n-d\ndocument to send in request body\n\n\n## Piping utils\n\n### Tee\n\nNamed after a plumbing \"T\" junction, tee allows us to pipe the output of a command to be redirected to two different commands, or to an output file and the screen.\n\nExamples:\n\n```bash\n# 1. redirect output to screen (stdout) and a file\nls -la | tee output.txt\n\n# 2. redirect output to a file and another command\nls -la |  tee output1.txt | head \n\n# 3. redirect output to two different files\nls -la |  tee output1.txt | awk '{print $1}' > output2.txt\n\n# It can be useful for creating backup files\ncat myfile.txt | tee myfile.bak | sed...\n```\n\n\n### Sponge\n\nPart of the `moreutils` package.\n\nSponge delays a file's modification, by creating a temporary buffer that allows using the same file as both input and output of a pipe.\n\nFor example:\n\n```bash\ncat myfile.txt | awk '{print $1}'> myfile.txt\n```\n\ndoes not work, because \"myfile.txt\" gets overwritten before awk is finished processing it. The normal way around it is to write the output to a temporary file, then replace the original file with the temporary.\n\nOn the other hand, we could use sponge:\n\n```bash\ncat myfile.txt | awk `{print $1}` | sponge > myfile.txt\n```","n":0.064}}},{"i":24,"$":{"0":{"v":"Tricks","n":1},"1":{"v":"\n## Networking\n\nFind out which process is listening on a given port:\n\n    [sudo] lsof -i :6600\n\n## File Transfers\n\nTransfer files from a remote host to a local machine with progress and compression:\n\n    rsync -azP remote-user@remote-host:~/Remote/Path ~/Destination\n\n## System\n\nFind number of processors:\n\n    nproc\n\nFind your private IP address:\n\n```\npython -c \"import socket; print(socket.gethostbyname(socket.gethostname()))\"\n```\n\nFind your public IP address:\n\n    curl ifconfig.io\n\n## Troubleshooting\n\nBurn an ISO image to a usb drive:\n\n    dd bs=4M if=path/to/image.iso of=/dev/sdx status=progress oflag=sync\n\n## File operations\n\nChange permissions by type [directory/file] using `find`:\n\n    sudo find [directory] -type [d/f] -exec chmod [privilege] {} \\;\n\n## Searching / Replacing\n\nRecursive find-and-replace all instances of a string in a directory:\n\n    find <mydir> -type f -exec sed -i 's/<string1>/<string2>/g' {} +\n\nInline search and replace:\n\n    echo \"${WORD/SEARCH/REPLACE}\"\n\nGrep PDF files:\n\n    find /path -iname '*.pdf' -exec pdfgrep pattern {} +\n\n## Images\n\nConvert format for multiple images:\n\n```bash\nfor file in *.jpg;\n  do convert $file ${file/.jpg/.png};\ndone\n```\nResize multiple images:\n\n```bash\nfor file in *.jpg;\n  do convert -quality  90 -resize 1000x1000 $file $file;\ndone\n```\n\nCrop multiple images (\"gravity\" = north, south, east, west, center):\n\n```bash\nfor file in *jpg;\n  do convert -extent 1000x1000 -gravity center $file $file;\ndone\n```\n\nOCR an image:\n\n    tesseract my-image output.txt\n\n## Video\n\nCreate a video from images in a folder. (Image file names must be indexed in order.)\n\n```\nffmpeg -framerate 30 -pattern_type glob -i '*.jpg' -c:v libx264 \\\n  -r 30 -pix_fmt yuv420p out.mp4\n```\n\n## More PDF Stuff\n\nConvert markdown to pdf:\n\n    pandoc -f my-notes.md -o my-notes.pdf\n\nOCR:\n\n    ocrmypdf original-document.pdf new-document.pdf\n\n","n":0.068}}},{"i":25,"$":{"0":{"v":"Tmux","n":1}}},{"i":26,"$":{"0":{"v":"Systemd","n":1},"1":{"v":"\nKnowing systemd is useful for automating many tasks in Linux.\n\n## Running Services as User\n\nRunning services as a user instead of root is beneficial in many cases.\n\nFor example, we may want to run Music Player Daemon (mpd) on startup as a user in order to use user configuration files instead of the global ones. In this case, we must first disable the default service, and re-enable it as user:\n\n    systemctl stop mpd.service\n    systemctl disable mpd.service\n    systemctl --user enable mpd.service\n\n## Creating Systemd Services\n\nUser services are stored under: `~/.config/systemd/user/`. Place any custom scripts that you write in this folder.\n\nHere is an example change-GTK-theme.service, which changes my computer's GTK theme to dark mode:\n\n```systemd\n[Unit]\nDescription=Change the GTK theme to dark mode.\nAfter=graphical.target\n\n[Service]\nType=oneshot\nExecStart=/bin/sh -c 'gsettings set org.gnome.desktop.interface gtk-theme Adwaita-dark && gsettings set org.gnome.gedit.preferences.editor scheme builder-dark'\n\n[Install]\nWantedBy=default.target\n```\n\n## Scheduling Systemd Tasks\n\nSystemd can be used as a more powerful alternative to cron bs.\n\nTo do this, you need to create two files:  `my-service.service`, and a `my-service.timer`, both files must have the same root name.\n\nExample timer file:\n\n```systemd\n[Unit]\nDescription=Change the GTK theme daily at a given time.\n\n[Timer]\nOnCalendar=*-*-* 16:00:00\nPersistent=true\n\n[Install]\nWantedBy=timers.target\n```\nThis timer runs the change-GTK.service shown previously every day at 16:00 hrs.\n\nFor more on timers: https://wiki.archlinux.org/index.php/Systemd/Timers\n\n\n## Viewing Systemd Logs\n\nYou can use journalctl to view logs from systemd.\n\n```bash\njournalctl -u my-service.service\n```\n\nSome useful commands:\n\n```bash\n# Show of the last 10 lines of a log every second\nwatch -n 1 journalctl -u myservice.service -n 10 --no-pager\n\n# Follow the log. Similar to above, but keeps all new history\njournalctl -f -u myservice.service\n\n# See all logs from the last hour\njournalctl -u myservice.service --no-pager --since=\"1 hour ago\"\n\n# Bonus: colorize the output using ccze\njournalctl -f -u myservice.service | ccze -o nolookups\n```\n","n":0.062}}},{"i":27,"$":{"0":{"v":"SSH","n":1},"1":{"v":"\nSSH (Secure Shell) provides a secure way to access another computer.\n\n## .ssh/config\n\nThe SSH configuration file `~/.ssh/config` is one of the most useful files for automating ssh connections. It allows you to create aliases, and saves you from having to remember and specify all the credentials and options every time you establish a connection.\n\n### Basic connection template\n\n```\nHost connection_name\n    HostName ssh.example.com\n    User username\n    Port 22\n```\n\n### Other useful settings\n\nPublic key authentication:\n\n`IdentityFile /home/user/.ssh/id_rsa`\n\nPort forwarding:\n\n`DynamicForward port_number`\n\nMultiple SSH hops:\n\n```\nProxyCommand ssh scripps -W %h:%p\nLogLevel=error\nRequestTTY force\n```\n\nExecute a command on login (e.g. change user):\n\n`RemoteCommand sudo su username`","n":0.107}}},{"i":28,"$":{"0":{"v":"jq","n":1},"1":{"v":"\nJSON (JavaScript Object Notation) is a common structured data format. It is often used as a replacement for XML, to transmit data between a server and a browser. JSON transmits human-readable data as objects composed of attribute-value pairs.\n\nExample JSON (https://www.w3schools.com/Js/json_demo.txt):\n\n```json\n{\n    \"name\":\"John\",\n    \"age\":31,\n    \"pets\":[\n        { \"animal\":\"dog\", \"name\":\"Fido\" },\n        { \"animal\":\"cat\", \"name\":\"Felix\" },\n        { \"animal\":\"hamster\", \"name\":\"Lightning\" }\n    ]\n}\n```\n\nHere, we take a look at the **jq** UNIX tool for manipulating JSON data. \n\nTo get started, we can pipe the json text above into `jq` using `curl`:\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | jq\n```\n\n## Selecting Attributes\n\nSelecting a single scalar value:\n\n```bash\n curl -s https://www.w3schools.com/Js/json_demo.txt | jq '.name'\n```\n\n```json\n\"John\"\n```\n\nSelecting multiple values:\n\n```bash\n curl -s https://www.w3schools.com/Js/json_demo.txt | \\\n jq '.name, .age'\n```\n\n```json\n\"John\"\n31\n```\n\nSelecting a single value from an array (index 0 returns the first element)\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | jq '.pets[0]'\n```\n\n```json\n{\n  \"animal\": \"dog\",\n  \"name\": \"Fido\"\n}\n```\n\nMore complicated queries can be achieved by combining dot operators, or using a pipe \"|\".\n\n```bash\n# Multiple dot operations\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq '.pets[0].name'\n\n# Same query using a pipe\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq '.pets[0] | .name'\n\n```\n\n```json\n\"Fido\"\n```\n\nSelecting elements from an array when an attribute matches a condition:\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq '.pets | map(select(.animal == \"hamster\"))'\n```\n\n```json\n[\n  {\n    \"animal\": \"hamster\",\n    \"name\": \"Lightning\"\n  }\n]\n```\n\nSelect also supports boolean operators:\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq '.pets | map(select(.animal == \"hamster\" or .animal == \"cat\"))'\n```\n\n```json\n[\n  {\n    \"animal\": \"cat\",\n    \"name\": \"Felix\"\n  },\n  {\n    \"animal\": \"hamster\",\n    \"name\": \"Lightning\"\n  }\n]\n```\n\n\n## Formatting Output\n\nSelected attributes can be formatted:\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq '.name + \" is \" + (.age | tostring)'\n```\n\n```json\n\"John is 31\"\n```\n\n## Editing JSON\n\nReplace a value:\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq ' .name = \"Jessica\" '\n```\n\n```json\ni{\n  \"name\": \"Jessica\",\n  \"age\": 31,\n  \"pets\": [\n    {\n      \"animal\": \"dog\",\n      \"name\": \"Fido\"\n    },\n    {\n      \"animal\": \"cat\",\n      \"name\": \"Felix\"\n    },\n    {\n      \"animal\": \"hamster\",\n      \"name\": \"Lightning\"\n    }\n  ]\n}\n```\n\nAdding a new key-value pair:\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq ' .pets += [{ \"animal\" : \"piggy\", \"name\": \"Chrorizo\" }] '\n```\n\n```json\n{\n  \"name\": \"John\",\n  \"age\": 31,\n  \"pets\": [\n    {\n      \"animal\": \"dog\",\n      \"name\": \"Fido\"\n    },\n    {\n      \"animal\": \"cat\",\n      \"name\": \"Felix\"\n    },\n    {\n      \"animal\": \"hamster\",\n      \"name\": \"Lightning\"\n    },\n    {\n      \"animal\": \"piggy\",\n      \"name\": \"Chrorizo\"\n    }\n  ]\n}\n```\nMore extensive modifications:\n\n```bash\ncurl -s https://www.w3schools.com/Js/json_demo.txt | \\\njq ' .pets += [{ \"animal\" : \"piggy\", \"name\": \"Chrorizo\" } ] | \n     .pets[0] += {\"breed\" : \"Chihuahua\" } |\n     .name = \"Ana\" |\n     .age = 28 |\n     .hobbies = \"ukulele\" '\n```\n\n```json\n{\n  \"name\": \"Ana\",\n  \"age\": 28,\n  \"pets\": [\n    {\n      \"animal\": \"dog\",\n      \"name\": \"Fido\",\n      \"breed\": \"Chihuahua\"\n    },\n    {\n      \"animal\": \"cat\",\n      \"name\": \"Felix\"\n    },\n    {\n      \"animal\": \"hamster\",\n      \"name\": \"Lightning\"\n    },\n    {\n      \"animal\": \"piggy\",\n      \"name\": \"Chrorizo\"\n    }\n  ],\n  \"hobbies\": \"ukulele\"\n}\n```\n\n","n":0.05}}},{"i":29,"$":{"0":{"v":"Flatpak","n":1}}},{"i":30,"$":{"0":{"v":"Docker","n":1},"1":{"v":"\n## Installing Docker\n\nInstallation steps have changed quite frequently with new releases of Docker. I will do my best to keep this up-to date, but no guarantees. Check your current release's instructions.\n\n### Ubuntu installation (July 2020)\n\nOlder versions of Docker were called docker, docker.io, or docker-engine. Now, the packages to install are docker-ce, docker-ce-cli, and containerd.io.\n\nOfficial documentation:\n\n[https://docs.docker.com/engine/install/ubuntu/](https://docs.docker.com/engine/install/ubuntu/)\n\nFor Ubuntu 20.04, follow instructions [here](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04).\n\n### Fedora installation (January 2020)\n\n    sudo dnf install docker\n    sudo sudo systemctl enable docker.service\n    sudo systemctl start docker.service\n\n**Note:** Fedora 31 had problems starting the docker daemon.\n\nDebugging with: `sudo dockerd --debug` shows \"Devices cgroup isn't mounted\". The solution was to modify the grub in order to start cgroup\n\n\n    sudo dnf install -y grubby\n    sudo grubby --update-kernel=ALL --args=\"systemd.unified_cgroup_hierarchy=0\"\n\nReboot computer.\n\nFinally, add user to the Docker group:\n\n    sudo groupadd docker && sudo gpasswd -a ${USER} docker\n\nLog out and in again to see changes.\n\n\n## Getting Docker Images\n\nPull from Docker Hub:\n\n    docker pull hub.docker.com/r/microsoft/powershell\n\nWe may need to specify a tag, e.g.:\n\n    docker pull ouchc/relion3.1:ubuntu16.04\n\nSearch for images: `docker search`\n\nSee installed images: `docker images`\n\n## Running Containers\n\nUse `docker run` to initially run a container.\n\n### Common options:\n\n-d\n: run in background (daemonize)\n\n-i\n: interactive\n\n-t\n: tty\n\n-p\n: make a port available outside container (**HOST:CONTAINER**)\n\n-v\n: mount a volume\n\n--name\n: assign container a name\n\n--rm\n: remove container when it exits\n\n## Interacting with containers\n\n### Basic commands\n\nOnce a container is running, you can easily start or stop it.\n\nStart a container (can use an assigned name): `docker start`\n\nStop a container `docker stop`\n\nExecute a command off a running container: `docker exec`\n\nLook inside container: `docker logs`\n\n### Connect to a container as root\n\n`docker exec -u 0 -it container_id /bin/bash`\n\n## Listing Containers in Docker\n\nTo show only running containers use:\n\n    docker container ls\n\nThe old, shorter way of doing this is `docker ps`.\n\n### Options:\n\n-a\n: see all containers\n\n-s\n: see container sizes\n\n\n\n## Clearing Non-running Containers\n\nContainers that are not running are not taking any system resources besides disk space.\n\nThe docker run flag `--rm` will automatically remove a container when it exits.\n\nRemove a specific container: `docker rm 3e552code34a`\n\nRemoves all stopped containers: `docker container prune`\n\nClean unused containers, networks, images, and volumes: `docker system prune`\n\n## Mounting a Host Filesystem\n\nUse the `-v` flag:\n\n    docker run -v /host/directory:/container/directory -other -options image_name command_to_run\n\n\n### Fix for permissions issues\n\nIn Fedora, SELinux can cause issues with mounting volumes.\nThe solution is to issue a SELinux rule:\n`chcon -Rt svirt_sandbox_file_t /path/to/volume`\n\n\nThis got easier recently since Docker finally merged a patch in docker-1.7.\n\nThis patch adds support for \"z\" and \"Z\" as options on the volume mounts (-v).\n\nFor example:\n\n    docker run -v /var/db:/var/db:z rhel7 /bin/sh\n\nWill automatically do the `chcon -Rt svirt_sandbox_file_t /var/db`.\n\nEven better, you can use Z.\n\n    docker run -v /var/db:/var/db:Z rhel7 /bin/sh\n\nThis will label the content inside the container with the exact MCS label that the container will run with, basically it runs `chcon -Rt svirt_sandbox_file_t -l s0:c1,c2 /var/db`,\nwhere s0:c1,c2 differs for each container.\n\nFor more info: [https://docs.docker.com/storage/volumes/](https://docs.docker.com/storage/volumes/)\n\n**e.g.** to run my powershell docker container with a mounted home directory:\n\n```\ndocker run -v /home/ravila/:/home/ravila/:Z -it mcr.microsoft.com/powershell\n```\n\n## Mapping Network Ports\n\n    docker run --name mycontainer -p 8080:8080 -p 8000:8000\n\n## Saving Changes to an Image\n\nIf you change anything (like install new packages) in the running container and exit the container the changes are not automatically saved. If you want to save them in an image, **use `docker commit`**.\n\n\n## Docker Files\n\nA simple example Dockerfile:\n\n```docker\n# Base image\nFROM fedora:latest\nRUN dnf -y update && dnf -y install unzip openssl  && dnf clean all\nCOPY UnblockNeteaseMusic-linux-arm7.zip /opt\nRUN unzip /opt/UnblockNeteaseMusic-linux-arm7.zip -d /opt/\nWORKDIR /opt/UnblockNeteaseMusic\nRUN  ./createCertificate.sh\nENTRYPOINT [\"/opt/UnblockNeteaseMusic/UnblockNeteaseMusic\", \" -b\", \" -e\"]\nEXPOSE 443\n```\n\nDocker file reference: https://docs.docker.com/engine/reference/builder/\n\nBest practices: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/\n\n\n### Build Dockerfile\n\nIn a directory with a Dockerfile run:\n\n`sudo docker build -t \"my-image\" .`\n\nIf the build is successful you can see  `my-image` in docker images output.\n\n## Docker Compose\n\nhttps://docs.docker.com/compose/\n\nhttps://developer.fedoraproject.org/tools/docker/compose.html\n\nDocker Compose is a tool to orchestrate Docker containers using a simple YAML file which describes your whole setup.\n\nInstallation: `sudo dnf install docker-compose`\n\n### Run a docker compose file\n\nCreate a YAML file: `docker-compose.yml`, then in the same directory, run:\n\n`docker-compose up`","n":0.04}}},{"i":31,"$":{"0":{"v":"dnf","n":1},"1":{"v":"\n## List installed repositories\n\n```bash\nyum repolist\nyum repolist disabled  # disabled repos\n```\n\n## Disable a repository\n\n\n```bash\ndnf config-manager --set-disabled repository\n```\n","n":0.25}}},{"i":32,"$":{"0":{"v":"Bash","n":1}}},{"i":33,"$":{"0":{"v":"Scripts","n":1},"1":{"v":"\nThis document collects basic syntax and best practices for writing\nBASH scripts.\n\n## Starting scripts\n\nIt is good practice to start BASH scripts with the following lines:\n\n```bash\n#!/bin/bash\n\nset -euo pipefail\n```\n### Set\n\nThe command `set` allows you to change shell options. The most useful flags are:\n\n-e\n: Stop execution when an error is encountered.\nYou can \"catch\" an error to prevent the shell from exiting by appending `|| true` after a failing command.\n\n-o pipefail\n: Catch errors in pipes. Return zero exit status only if all commands in a pipeline succeed.\n\n-u\n: Error out when shell encounters undefined variables by their values.\n\n-x\n: Debug mode. Prints every line and output, substituting variables\n\n-f\n: Disable file globbing.\n\n## A general template for BASH scripts\n\nThis template contains the most common boilerplate code for a CLI application:\n\n```bash\n\n```bash\n\n#!/usr/bin/env bash\n\nset -Eeuo pipefail\n\ncd \"$(dirname \"${BASH_SOURCE[0]}\")\" >/dev/null 2>&1\n\ntrap cleanup SIGINT SIGTERM ERR EXIT\n\nusage() {\n  cat <<EOF\nUsage: $(basename \"$0\") [-h] [-v] [-f] -p param_value arg1 [arg2...]\nScript description here.\nAvailable options:\n-h, --help      Print this help and exit\n-v, --verbose   Print script debug info\n-f, --flag      Some flag description\n-p, --param     Some param description\nEOF\n  exit\n}\n\ncleanup() {\n  trap - SIGINT SIGTERM ERR EXIT\n  # script cleanup here\n}\n\nsetup_colors() {\n  if [[ -t 2 ]] && [[ -z \"${NO_COLOR-}\" ]] && [[ \"${TERM-}\" != \"dumb\" ]]; then\n    NOCOLOR='\\033[0m' RED='\\033[0;31m' GREEN='\\033[0;32m' ORANGE='\\033[0;33m' BLUE='\\033[0;34m' PURPLE='\\033[0;35m' CYAN='\\033[0;36m' YELLOW='\\033[1;33m'\n  else\n    NOCOLOR='' RED='' GREEN='' ORANGE='' BLUE='' PURPLE='' CYAN='' YELLOW=''\n  fi\n}\n\nmsg() {\n  echo >&2 -e \"${1-}\"\n}\n\ndie() {\n  local msg=$1\n  local code=${2-1} # default exit status 1\n  msg \"$msg\"\n  exit \"$code\"\n}\n\nparse_params() {\n  # default values of variables set from params\n  flag=0\n  param=''\n\n  while :; do\n    case \"${1-}\" in\n    -h | --help)\n      usage\n      ;;\n    -v | --verbose)\n      set -x\n      ;;\n    --no-color)\n      NO_COLOR=1\n      ;;\n    -f | --flag) # example flag\n      flag=1\n      ;;\n    -p | --param) # example named parameter\n      param=\"${2-}\"\n      shift\n      ;;\n    -?*)\n      die \"Unknown option: $1\"\n      ;;\n    *)\n      break\n      ;;\n    esac\n    shift\n  done\n\n  args=(\"$@\")\n\n  # check required params and arguments\n  [[ -z \"${param-}\" ]] && die \"Missing required parameter: param\"\n  [[ ${#args[@]} -eq 0 ]] && die \"Missing script arguments\"\n\n  return 0\n}\n\nparse_params \"$@\"\nsetup_colors\n\n# script logic here\n\nmsg \"${RED}Read parameters:${NOCOLOR}\"\nmsg \"- flag: ${flag}\"\nmsg \"- param: ${param}\"\nmsg \"- arguments: ${args[*]-}\"\n\n```\n\n\n\n","n":0.054}}},{"i":34,"$":{"0":{"v":"Regex","n":1},"1":{"v":"\n## Match Location\n\n| Action      | Regex     |\n| :---------- | :-------- |\n| Match start | `^string` |\n| Match end   | `string$` |\n\n## Datatypes\n\n| Action                  | Regex |\n| :---------------------- | :---- |\n| Match integer           | `\\d`  |\n| Match multiple integers | `\\d+` |\n","n":0.156}}},{"i":35,"$":{"0":{"v":"Git","n":1},"1":{"v":"\n## Managing Remotes\n\nAdd a named remote (you can have multiple remotes):\n\n```git\ngit remote add github https://github.com/user/repo.git\ngit remote add gitlab https://gitlab.com/user/repo.git\ngit push github\ngit push gitlab\n```\n\nListing remotes:\n```git\ngit remote -v\n```\n\nOverloading origin with another remote:\n\n```git\ngit remote set-url â€“â€“add origin https://github.com/user/repo.git\n```\n\nDeleting a remote:\n\n```git\ngit remote remove origin\n```\n\nSynchronizing a local fork with remote:\n\n```git\ngit remote add upstream https://github.com/OriginalRepo/OriginalProject.git\ngit merge upstream/master\ngit push origin master\n```\n\n## Submodules\n\nCreating submodules in an existing repo:\n\n```git\ngit submodule add https://github.com/repo.git\n```\n\nCloning a repository with submodules:\n\n```git\ngit clone https://github.com/repo.git\ngit submodule init\ngit sumbodule update\n```\n\n## Staging and Commits\n\nRemove a file or folder from the staging area:\n\n```git\ngit reset HEAD -- <file or folder>\n```\n\n## Branching\n\nDeleting a local branch:\n\n```git\ngit branch -d branch_name\n```\n\nDeleting a remote branch:\n\n```git\npush --delete remote_name branch\n```\n\nFetch all branches from a remote:\n\n```git\ngit fetch --all\n```\n\nCheckout specific files from another branch:\n\n```git\ngit checkout branch_name file file2\n```\n\nUpdate a single file from upstream:\n\n```git\ngit checkout origin/master --  file\n```\n\nMerge a specific commit to current branch:\n\n```git\ngit cherry-pick 63344f2\n```\n\nHide branch changes:\n\n```git\ngit stash\n```\n\nRestore branch changes:\n\n```git\ngit stash apply\n```\n\n## Workflows\n\n### Fetch a PR for local testing\n\nCheckout PR into a new branch.\n\n```git\ngit fetch upstream pull/{PR-NUMBER}/head:test-branch-name\n```\n\nMerge PR into local master branch, and test.\n\n```git\ngit checkout master\ngit merge test-branch-name\n```\n\nIf code was bad, revert to previous commit.\n\n```git\ngit reset --hard HEAD@{1}\n```\n\n\n### Move a commit to another repo\n\n```git\ngit config pull.rebase true\ngit show --pretty=email <commit> > patch\ngit am --committer-date-is-author-date < patch\n```\n\n### Finding things\n\nFind commits that contain a specific string:\n\n```git\ngit log -S 'your_search_string' --oneline --name-status\n```\n\nDisplay the contents of a commit:\n\n```git\ngit show <commit>\n```\n\n## Undoing Stuff and Fixing Mistakes\n\nUndo changes to a file:\n\n```git\ngit checkout -- <file>\n```\n\nUndo a merge, keeping local changes\n\n```git\ngit reset --merge ORIG_HEAD\n```\n\nUndo the last local commit\n\n```git\ngit reset --soft HEAD~1\n```\n\nUndo a commit pushed to GitHub deleting all history:\n\n```git\ngit reset --hard 93827927ed6e245be\ngit reset HEAD~1\ngit stash\ngit add .\ngit commit -m \"your new commit message here\"\ngit push --force\n```\n\nTo pull the new history to a repository that has diverged:\n\n```git\ngit pull --rebase\n```\n\n\n","n":0.06}}},{"i":36,"$":{"0":{"v":"Dendron","n":1},"1":{"v":"\nSome Dendron workflows that I use often, and need to remember.\n\n## Refactoring\n\n**Dendron: Move Header** - Move a section to a new note\n","n":0.213}}},{"i":37,"$":{"0":{"v":"Database","n":1}}},{"i":38,"$":{"0":{"v":"SQLite","n":1},"1":{"v":"\n## About SQLite\n\nMost SQL databases work with a client/server model. In contrast, SQLite databases operate from single, cross-platform portable files. They can be stored on various storage devices and can be transferred across different computers.\n\n## Basic Commands\n\nCreating a database from scratch and reading a pre-existing database file works the same way:\n\n    sqlite3 mydatabse.db\n\n**Meta Commands** in SQLite always start with a dot (`.`), and don't require a closing semicolon (`;`).\n\n| Action                    | Meta Command             |\n| :------------------------ | :----------------------- |\n| Help                      | `.help`                  |\n| Display tables            | `.tables`                |\n| Display table schema      | `.schema <table>`        |\n| Quit                      | `.quit`                  |\n| Set mode for output table | `.mode`                  |\n| Import data to a table    | `.import <file> <table>` |\n\n### SELECT\n\n``` sql\n-- Select everything\nSELECT * FROM my_table;\n\n-- Select columns\nSELECT column_a, column_b, column_d FROM my_table;\n\n-- Select rows\nSELECT * FROM my_table WHERE column_a = x;\n\n-- Select top 5 rows\nSELECT * FROM my_table LIMIT 5;\n```\n\n### DELETE\n\nDelete data from a table:\n\n``` sql\nDELETE FROM my_table WHERE column_a = x;\n```\n\n## Creating Tables\n\nSQLite uses **Manifest Typing**.\nManifest Typing releases many restrictions on the type of value that can be entered for a particular field. This allows you to enter any value of any datatype into a column, irrespective of the declared type of the column (except for INTEGER PRIMARY KEY).\n\n``` sql\nCREATE TABLE comments ( \n\tpost_id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, \n\tname TEXT NOT NULL, \n\temail TEXT NOT NULL, \n\twebsite_url TEXT NULL, \n\tcomment TEXT NOT NULL );\n```\n\nYou can also create a table from a csv file. If the table that you are importing into already exists, then the first row of the csv is interpreted as data. If it does not exist, it is created, and the first row of the csv is interpreted as a header.\n\n```sql\n-- Set import mode to csv\n.mode csv\n.import my_file.csv my_table\n```\n\n## Datatypes\n\nThere are five datatypes in SQLite3:\n\n- NULL\n- INTEGER\n- REAL\n- TEXT\n- BLOB\n\nSQLite3 does not have Boolean or Date and Time datatypes. Booleans are commonly stored as integers (1 or 0) and Date/time is stored as either text (ISO8601 strings), real numbers (Julian day numbers), or integers (UNIX time), using the built-in  date and time functions.\n\n## Deleting Tables\n\nDelete a table if it exists:\n\n```sql\nDROP TABLE IF EXISTS my_table\n```\n\n## Interfacing with Python\n\nWe can use the sqlite3 Python module to connect to a database, and read a table into a Pandas dataframe. \n\n```python\n# Connect to SQLite database\nconn = sqlite3.connect(\"my_database.db\")\nc = conn.cursor()\n\n# Read table into Pandas\ndf = pd.read_sql(\"select * from my_table\", conn)\n```\n\n","n":0.05}}},{"i":39,"$":{"0":{"v":"PostgreSQL","n":1},"1":{"v":"\n## Common navigation commands\n\nShow databases: `\\l`\n\nConnect to database: `\\c`\n\nDisplay tables: `\\d`\n\nShow columns: `\\d table`\n\nDescribe table: `\\d+ table`\n\n## Selections\n\n```sql\nSELECT * FROM table;\n```\n\n","n":0.218}}},{"i":40,"$":{"0":{"v":"MongoDB","n":1},"1":{"v":"\n## Installation\n\n### Docker:\n\n`docker run -d -p 27017-27019:27017-27019 --name mongodb mongo:latest`\n\n## Common navigation commands\n\nUse mongo CLI to connect to host:\n\n```\nmongo \"mongodb://hostname_or_ip:port\"\n```\n\nList databases\n>`show dbs`\n\nSelect database\n> `use <database_name>`\n\nList collections\n> `show collections`\n\nGet number of documents in collection\n> `db.collection.count()`\n\nGet example document from a collection\n> `db.collection.findOne()`\n\n## Searching\n\nSearch by attribute:\n\n```\ndb.collection.find({'field': 'value'})\n```\n\nFind documents with field:\n\n```\ndb.collection.find({'field': {$exists: true}})\n```\n\nSpecial operators for filters:\n\n* `$exists`\n* `$gt`, `$gte`, `$lt`, `$lte`\n* `$type`\n* `$and: [{expr}, {expr}]`\n* `$eq`\n\nIterate through search results:\n\n```\nmyCursor = db.collection.find({'field': 'value'});\nmyCursor.count();\nwhile (myCursor.hasNext()){\n    print(tojson(myCursor.next()));\n}\n```\n\nLimit to top `n` results:\n\n`.limit()`\n\n## Indexing\n\n### Create an index from a collection\n\nHash-based sharding. Supports equality matches, but not range-based queries.\n\n`db.chembl.createIndex({'chembl.inchi': 'hashed'})`\n\nAscending index:\n\n`db.chembl.createIndex({'chembl.inchi': 1})`\n\n## Updating documents\n\nUpdate single document:\n\n`db.collections.update({'_id': 'value'}, {update})`\n\nUpdate multiple documents:\n\n`db.collection.updateMany({filter}, {update})`\n\nFor modifiacations on specific attributes, use `$set` operator:\n\n`{$set: {'field': 'new_value'}}`\n\n## Inserting new documents\n\nInsert single document to a collection:\n\n`db.collection.insert({doc})`\n\n\n## Delete a collection\n\n`db.collection.drop()`\n\n## Rename a collection\n\n`db.collection.renameCollection(\"new_name\")`\n\n## Clone a collection\n\n`db.collection1.aggregate([{ $match: {} }, { $out: \"collection2\" }])`\n\n## Dumping and restoring a collection\n\nThis requires installing the mongodb [database tools package](https://www.mongodb.com/try/download/database-tools?tck=docs_databasetools).\n\n\n```\nmongodump --db=<old_db_name> --collection=<collection_name> --out=data/\n\nmongorestore --db=<new_db_name> \\\n  --collection=<collection_name> data/<db_name>/<collection_name>.bson\n```\n\n## Conditionals\n\n\n\n### Javascript conditionals\n\n```js\nif({condition}){\n    // code\n}\nelse {\n    //code\n}\n```\n","n":0.079}}},{"i":41,"$":{"0":{"v":"Kibana","n":1},"1":{"v":"\nKibana is a user interface that provides search and data visualization for data indexed in Elasticsearch, as well as management tools for an Elasticsearch cluster.\n\n## Installation\n\nFor a local Docker installation, we use the same docker-compose as Elasticsearch:\n\n![[database.elasticsearch#disabling-elasticsearch-security-settings,1:#*]]\n\nHowever, we add a service for Kibana. The benefit is that services in the same docker-compose file use the same virtual network, so they should discover and able to speak to each other out of the box. (No need to worry about the enrollment token and all that.)\n\nThe Kibana service looks like this:\n\n``` yml\nkibana:\n    container_name: kibana_8.1\n    image: docker.elastic.co/kibana/kibana:8.1.0\n    ports:\n      - 5601:5601\n```\n\n## Using Kibana\n\n### Index Management\n\nUseful for checking index status and configs.\n\n<http://localhost:5601/app/management/data/index_management/indices>\n\n### Dev tools\n\nUseful for sending custom queries.\n\n<http://localhost:5601/app/dev_tools#/console>","n":0.094}}},{"i":42,"$":{"0":{"v":"Elasticsearch","n":1},"1":{"v":"\n## API\n\nSome examples of common API uses.\n\n### Create index\n\n`curl -X PUT \"localhost:9200/user_genesets?pretty\"`\n\n### Delete index\n\n`curl -X DELETE \"localhost:9200/user_genesets?pretty\"`\n\n### Set index alias\n\n```bash\ncurl -XPOST 'http://localhost:9200/_aliases' -d '{\n    \"actions\" : [\n        { \"add\" : { \"index\" : \"mygeneset_20210323_k1udhp5n\",\n          \"alias\" : \"mygeneset_current\" } }\n    ]\n}' -H \"Content-Type: application/json\"\n```\n### See list of indices\n\n`curl -x GET 'http://localhost:9200/_cat/indices'`\n\n### Search for documents\n\nIn general, we use the endpoint:\n\n`curl -X GET \"localhost:9200/index/_search\"`\n\nfor example (the parameter 'pretty' requires return type `application/json`):\n\n```bash\ncurl -X GET \"es8.biothings.io:9200/my_index/_search?&pretty\" \\\n-H 'Content-Type: application/json' -d'\n{\n  \"query\": {\n    \"term\": {\n      \"source\": \"smpdb\"\n    }\n  }\n}\n'\n```\n\n### Insert document\n\n### Modify document\n\n### Delete document","n":0.105}}},{"i":43,"$":{"0":{"v":"Installation","n":1},"1":{"v":"\n\n## Installation\n\n### Docker image\n\nElasticsearch can be easily installed using [[linux.docker]].\n\nFor ElasticSearch 8:\n\n```bash\ndocker network create elastic\ndocker pull docker.elastic.co/elasticsearch/elasticsearch:8.1.0\ndocker run -d -p 9200:9200 -p 9300:9300 --name elasticsearch_8 \\\n  --net elastic -t docker.elastic.co/elasticsearch/elasticsearch:8.1.0\n```\n\n### Disabling Elasticsearch Security Settings\n\nWe must disable some security settings to easily connect to our container locally (we leave them on in prod). The settings to disable are: `xpack.security.enabled`, `xpack.security.http.ssl`, and `xpack.security.transport.ssl`.\n\nI found the easiest method is to create a [[docker-compose|linux.docker#docker-compose,1:#*]] file and bind-mount a config file:\n\n```yml\nversion: '3'\nservices:\n  elasticsearch:\n    container_name: elasticsearch_8.1\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.1.0\n    ports:\n      - 9200:9200\n      - 9300:9300\n    volumes:\n      - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\n```\n\n`elasticsearch.yml` has the following settings:\n\n```yml\ncluster.name: \"docker-cluster\"\nnode.name: node1\nnetwork.host: 0.0.0.0\nxpack.security.enabled: false\nxpack.security.enrollment.enabled: true\nxpack.security.http.ssl:\n  enabled: false\nxpack.security.transport.ssl:\n  enabled: false\ncluster.initial_master_nodes:\n  - node1\n\n```\n\n### Virtual Memory\n\nI ran into a virtual memory problem:\n\n> Elasticsearch: Max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n\nSolution:\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html\n\n```bash\nsysctl -w vm.max_map_count=262144\n```\n\nTo modify this setting permanently, edit `/etc/sysctl.conf` and set `vm.max_map_count` to  262144.\n\n### Connect to ES using CA certs\n\nInstall Docker image, but don't disable security settings.\n\nCopy the CA certs to the host machine:\n\n```\ncp elasticsearch_8:/usr/share/elasticsearch/config/certs/http_ca.crt .\n\n```\n\nYou can store the cert file in `/etc/ssl/certs` for system-wide access.\n\nConnection string with simple authentication:\n\n```\ncurl --cacert http_ca.crt -u elastic:<password> https://localhost:9200\n```\n\nCreate an API key:\n\n```\ncurl -X POST --cacert http_ca.crt -u elastic:<password> \\\n  https://localhost:9200/_security/api_key \\\n  -H 'Content-Type: application/json' -d'{\"name\": \"my_api_key\"}'\n```\nThe API key is the 'encoded' value in the JSON response.\n\nTo connect with an API key:\n\n```\ncurl --cacert http_ca.crt -H \\\n  \"Authorization: ApiKey <encoded_api_key>\" https://localhost:9200\n```","n":0.067}}},{"i":44,"$":{"0":{"v":"Data Processing","n":0.707}}},{"i":45,"$":{"0":{"v":"XML","n":1},"1":{"v":"\n# XML Transformations\n\nFree tester tool: https://xslttest.appspot.com/\n\n## Sorting XML Records with XSLT\n\nhttps://dellboomi.force.com/community/s/article/Sorting-XML-Records-with-XSLT\n\nLet's say that we have an XML document that looks like this:\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<entityList>\n    <createdDate>2020-03-02T05:02:03.000-08:00</createdDate>\n    <entity internalId=\"376531302\" lastModifiedDate=\"2020-03-03T05:51:35.000-08:00\">\n        <entityName>99001 ABC - Entity A</entityName>\n    </entity>\n    <entity internalId=\"37651304\" lastModifiedDate=\"2020-03-02T04:37:24.000-08:00\">\n        <entityName>99006 ABC - Entity D</entityName>\n    </entity>\n    <entity internalId=\"376531302\" lastModifiedDate=\"2020-03-02T04:19:16.000-08:00\">\n        <entityName>99005 ABC - Entity C</entityName>\n    </entity>\n    <entity internalId=\"376531302\" lastModifiedDate=\"2020-03-20T06:44:22.000-08:00\">\n        <entityName>99002 ABC - Entity B</entityName>\n    </entity>\n</entityList>\n```\n\nThis is the final processed form of our data, but the user we're sending this to has asked that we sort it by the \"elementName\" value.\nWe'll use these two templates in an XSLT Stylesheet for that.\n\n```xml\n<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n<xsl:output method=\"xml\" indent=\"yes\" />\n\n     <xsl:template match=\"entityList\">\n        <xsl:copy>\n            <xsl:apply-templates select=\"@*\"/>\n            <xsl:apply-templates select=\"entity\">\n                <xsl:sort select=\"entityName\" order=\"ascending\"/>\n            </xsl:apply-templates>\n        </xsl:copy>\n    </xsl:template>\n\n    <xsl:template match=\"@* | node()\">\n        <xsl:copy>\n            <xsl:apply-templates select=\"@* | node()\"/>\n        </xsl:copy>\n    </xsl:template>\n\n</xsl:stylesheet>\n```","n":0.089}}},{"i":46,"$":{"0":{"v":"BioThings","n":1}}},{"i":47,"$":{"0":{"v":"Translator","n":1}}},{"i":48,"$":{"0":{"v":"Biolink Model","n":0.707},"1":{"v":"\nhttps://biolink.github.io/biolink-model/\n\n## CURIEs\n\nA CURIE (or Compact URI) is an abbreviated syntax for expressing Uniform Resource Identifiers (URIs).\n\nAn example of CURIE syntax: [isbn:0393315703]\n\nwhere the `isbn` prefix refers to the URI:\n\nhttps://cthoyt.com/2021/09/14/curies.html\n\nBiology-related CURIEs are registered by identifiers.org:\n\nhttps://registry.identifiers.org/registry\n\nOther resources:\n- https://bioregistry.io/registry/\n\nBiolink model:\n- https://github.com/biolink/biolink-model/blob/master/biolink-model.yaml\n- https://github.com/biolink/biolink-model/blob/master/context.jsonld\n- https://nodenormalization-sri.renci.org/1.2/get_curie_prefixes","n":0.16}}},{"i":49,"$":{"0":{"v":"BioThings SDK","n":0.707}}},{"i":50,"$":{"0":{"v":"Hub Shell","n":0.707},"1":{"v":"\n## Connecting to the shell\n\n## Indexing a build\n\nIndex, while deleting previous indices:\n\n```python\n index(\"local8\", \"geneset_20220902_5sw4ptjd\", mode=\"purge\")\n```","n":0.258}}},{"i":51,"$":{"0":{"v":"Using Biothings SDK with ES8","n":0.447},"1":{"v":"\n\nInstall ES8 Docker image,\n\n![[database.elasticsearch#docker-image,1:#*]]\n\nMake note of the username and password provided.\n\nCopy the CA certs to the host machine:\n\n```\ncp elasticsearch_8:/usr/share/elasticsearch/config/certs/http_ca.crt .\n\n```\n\nYou can now connect using simple username/password authentication:\n\n```\ncurl --cacert http_ca.crt -u elastic:<password> https://localhost:9200\n```\n\n\n## Configuring Web for ES8\n\nTo connect to ES8 using SSL certificates, set these arguments in config.py\n\n```python\nES_ARGS = {\n    \"use_ssl\": True,\n    \"verify_certs\": True,\n    \"ca_certs\": \"http_ca.crt\",\n    \"api_key\": encoded_api_key\"\n}\n```\n\n`encoded_api_key` is a one-time API key that can be generated by the following command:\n\n```\ncurl -X POST --cacert http_ca.crt -u elastic:<password>\n  https://localhost:9200/_security/api_key \\\n  -H 'Content-Type: application/json' -d'{\"name\": \"my_api_key\"}'\n```\n\n## Configuring Hub for ES8\n\nSimilar to web, to configure the hub, we ned to set `use_ssl`, `verify_certs`, `ca_certs`, and `api_key` for the `INDEX_CONFIG` settings under `indexer.args`.","n":0.096}}},{"i":52,"$":{"0":{"v":"Bioinformatics","n":1}}},{"i":53,"$":{"0":{"v":"Genomics","n":1}}},{"i":54,"$":{"0":{"v":"Genomics Vocabulary","n":0.707}}},{"i":55,"$":{"0":{"v":"Blast","n":1}}},{"i":56,"$":{"0":{"v":"Evalues","n":1},"1":{"v":"\n## E-value\n\nThe BLAST E-value is the number of expected hits of similar quality (score) that could be found just by chance.\n\nE-value of 10 means that up to 10 hits can be expected to be found just by chance, given the same size of a random database.\n\nE-value can be used as a first quality filter for the BLAST search result, to obtain only results equal to or better than the number given by the `-evalue` option. Blast results are sorted by E-value by default (best hit in first line).\n\n\n\n    blastn -query genes.ffn -subject genome.fna -evalue 1e-10\n\n\nThe smaller the E-value, the better the match. Here is a rule of thumb:\n\n`-evalue 1e-50`\n: small E-value: low number of hits, but of high quality. Blast hits with an E-value smaller than 1e-50 includes database matches of very high quality.\n\n`-evalue 0.01`\n: Blast hits with E-value smaller than 0.01 can still be considered as good hit for homology matches.\n\n`-evalue 10 (default)`\n: Large E-value: many hits, partly of low quality\n\nE-value smaller than 10 will include hits that cannot be considered as significant, but may give an idea of potential relations.\n\nThe E-value (expectation value) is a corrected bit-score adjusted to the sequence database size. The E-value therefore depends on the size of the used sequence database. Since large databases increase the chance of false positive hits, the E-value corrects for the higher chance. It's a correction for multiple comparisons. This means that a sequence hit would get a better E-value when present in a smaller database.\n\n$E = m \\cdot n / 2^{bit-score}$\n\n\n $m$ - query sequence length\n\n $n$ - total database length (sum of all sequences)\n\n## Bit-score\n\n> The higher the bit-score, the better the sequence similarity\n\n >The bit-score is the required size of a sequence database in which the current match could be found just by chance. The bit-score is a log2 scaled and normalized raw-score. Each increase by one doubles the required database size (2bit-score).\n\n> Bit-score does not depend on database size. The bit-score gives the same value for hits in databases of different sizes and hence can be used for searching in an constantly increasing database.\n\nFrom <http://www.metagenomics.wiki/tools/blast/evalue>\n\n---\n\n> The E-value provides information about the likelihood that a given sequence match is purely by chance. The lower the E-value, the less likely the database match is a result of random chance and therefore the more significant the match is. Empirical interpretation of the E-value is as follows. If E < 1e - 50 (or 1 Ã— 10-50), there should be an extremely high confidence that the database match is a result of homologous relationships. If E is between 0.01 and 1e - 50, the match can be considered a result of homology. If E is between 0.01 and 10, the match is considered not significant, but may hint at a tentative remote homology relationship. Additional evidence is needed to confirm the tentative relationship. If E > 10, the sequences under consideration are either unrelated or related by extremely distant relationships that fall below the limit of detection with the current method. Because the E-value is proportionally affected by the database size, an obvious problem is that as the database grows, the E-value for a given sequence match also increases.\n\n> A bit score is another prominent statistical indicator used in addition to the Evalue in a BLAST output. The bit score measures sequence similarity independent of query sequence length and database size and is normalized based on the rawpairwise alignment score. The bit score (S) is determined by the following formula: S = (Î» Ã— S âˆ’ lnK)/ ln2 where Î» is the Gumble distribution constant, S is the raw alignment score, and K is a constant associated with the scoring matrix used. Clearly, the bit score (S) is linearly related to the rawalignment score (S). Thus, the higher the bit score, the more highly significant the match is. The bit score provides a constant statistical indicator for searching different databases of different sizes or for searching the same database at different times as the database enlarges.\n\nFrom <https://www.biostars.org/p/187230/>\n\n---\n\n**Relevant article:\n**https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3820096/\n\n> The bit-score provides a better rule-of-thumb for inferring homology. For average length proteins, a bit score of 50 is almost always significant. A bit score of 40 is only significant (E() < 0.001) in searches of protein databases with fewer than 7000 entries. Increasing the score by 10 b\n\n\nFrom <https://www.biostars.org/p/187230/>","n":0.037}}},{"i":57,"$":{"0":{"v":"AWS","n":1}}},{"i":58,"$":{"0":{"v":"R","n":1},"1":{"v":"\n## R Basics\n\n### Installing packages\n\n```R\ninstall.packages(\"package_name\")\n```\n\n### Useful packages\n\n- `languageserver`: Provides code completions and syntax highlighting for IDEs.\n- `r-httpgd`: Used by vscode to display plots in the webview.\n- `devtools`: R packaging utilities. e.g. Install packages from GitHub.\n\n\n### R tools\n\n- **radian**: A replacement R shell, with nice syntax highlighting and completion, like ipython.","n":0.141}}},{"i":59,"$":{"0":{"v":"MacOS","n":1}}},{"i":60,"$":{"0":{"v":"Tricks","n":1},"1":{"v":"\n## Enable Key Repeat\n\nRun this and reboot:\n\n```bash\ndefaults write -g ApplePressAndHoldEnabled -bool false\n```\n\n","n":0.289}}}]}
